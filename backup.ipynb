{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "sc_vuln_8label = pd.read_csv(\"SC_Vuln_8label.csv\")\n",
    "sc_4label = pd.read_csv(\"SC_4label.csv\")\n",
    "\n",
    "# Print original dataset sizes\n",
    "print(f\"Original SC_Vuln_8label.csv rows: {len(sc_vuln_8label)}\")\n",
    "print(f\"Original SC_4label.csv rows: {len(sc_4label)}\")\n",
    "\n",
    "# Define mapping from SC_4label to SC_Vuln_8label\n",
    "label_mapping = {\n",
    "    \"/content/drive/My Drive/SC_Dataset/dangerous delegatecall (DE)/\": \"./Dataset/dangerous delegatecall (DE)/\",\n",
    "    \"/content/drive/My Drive/SC_Dataset/integer overflow (OF)/\": \"./Dataset/integer overflow (OF)/\",\n",
    "    \"/content/drive/My Drive/SC_Dataset/reentrancy (RE)/\": \"./Dataset/reentrancy (RE)/\",\n",
    "    \"/content/drive/My Drive/SC_Dataset/timestamp dependency (TP)/\": \"./Dataset/timestamp dependency (TP)/\"\n",
    "}\n",
    "\n",
    "label_encoded_mapping = {\n",
    "    \"./Dataset/dangerous delegatecall (DE)/\": 1,\n",
    "    \"./Dataset/integer overflow (OF)/\": 4,\n",
    "    \"./Dataset/reentrancy (RE)/\": 5,\n",
    "    \"./Dataset/timestamp dependency (TP)/\": 6\n",
    "}\n",
    "\n",
    "# Update labels in SC_4label.csv\n",
    "sc_4label[\"label\"] = sc_4label[\"label\"].map(label_mapping)\n",
    "sc_4label[\"label_encoded\"] = sc_4label[\"label\"].map(label_encoded_mapping)\n",
    "\n",
    "# Merge datasets\n",
    "merged_df = pd.concat([sc_vuln_8label, sc_4label], ignore_index=True)\n",
    "print(f\"Total rows after merging (before removing duplicates): {len(merged_df)}\")\n",
    "\n",
    "# Find duplicate rows based on filename and label_encoded\n",
    "duplicates = merged_df[merged_df.duplicated(subset=[\"filename\", \"label_encoded\"], keep=\"first\")]\n",
    "\n",
    "# Print and save duplicate rows for manual inspection\n",
    "print(f\"Duplicate rows detected: {len(duplicates)}\")\n",
    "duplicates.to_csv(\"Duplicate_Rows.csv\", index=False)\n",
    "print(\"Duplicate rows saved as 'Duplicate_Rows.csv' for manual inspection.\")\n",
    "\n",
    "# Remove duplicates\n",
    "merged_no_duplicates = merged_df.drop_duplicates(subset=[\"filename\", \"label_encoded\"], keep=\"first\")\n",
    "\n",
    "# Print final dataset size\n",
    "print(f\"Total rows after removing duplicates: {len(merged_no_duplicates)}\")\n",
    "print(f\"Number of duplicate rows removed: {len(merged_df) - len(merged_no_duplicates)}\")\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_no_duplicates.to_csv(\"Merged_SC_Dataset.csv\", index=False)\n",
    "\n",
    "print(\"Merging complete! Saved as 'Merged_SC_Dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "sc_vuln_8label = pd.read_csv(\"SC_Vuln_8label.csv\")\n",
    "sc_4label = pd.read_csv(\"SC_4label.csv\")\n",
    "\n",
    "# Define mapping from SC_4label to SC_Vuln_8label\n",
    "label_mapping = {\n",
    "    \"/content/drive/My Drive/SC_Dataset/dangerous delegatecall (DE)/\": \"./Dataset/dangerous delegatecall (DE)/\",\n",
    "    \"/content/drive/My Drive/SC_Dataset/integer overflow (OF)/\": \"./Dataset/integer overflow (OF)/\",\n",
    "    \"/content/drive/My Drive/SC_Dataset/reentrancy (RE)/\": \"./Dataset/reentrancy (RE)/\",\n",
    "    \"/content/drive/My Drive/SC_Dataset/timestamp dependency (TP)/\": \"./Dataset/timestamp dependency (TP)/\"\n",
    "}\n",
    "\n",
    "label_encoded_mapping = {\n",
    "    \"./Dataset/dangerous delegatecall (DE)/\": 1,\n",
    "    \"./Dataset/integer overflow (OF)/\": 4,\n",
    "    \"./Dataset/reentrancy (RE)/\": 5,\n",
    "    \"./Dataset/timestamp dependency (TP)/\": 6\n",
    "}\n",
    "\n",
    "# Update labels in SC_4label.csv\n",
    "sc_4label[\"label\"] = sc_4label[\"label\"].map(label_mapping)\n",
    "sc_4label[\"label_encoded\"] = sc_4label[\"label\"].map(label_encoded_mapping)\n",
    "\n",
    "# Merge datasets\n",
    "merged_df = pd.concat([sc_vuln_8label, sc_4label], ignore_index=True)\n",
    "\n",
    "# 找出 filename 和 label_encoded 相同，但 code 可能不同的行\n",
    "duplicates = merged_df[merged_df.duplicated(subset=[\"filename\", \"label_encoded\"], keep=False)]\n",
    "\n",
    "# 按 filename 和 label_encoded 分组，把 code 放到列表\n",
    "comparison_df = duplicates.groupby([\"filename\", \"label_encoded\"])[\"code\"].apply(list).reset_index()\n",
    "\n",
    "# 确保每个 group 只有两个 code 值，超出部分丢弃，少于两个的填充 NaN\n",
    "comparison_df[\"code_1\"] = comparison_df[\"code\"].apply(lambda x: x[0] if len(x) > 0 else None)\n",
    "comparison_df[\"code_2\"] = comparison_df[\"code\"].apply(lambda x: x[1] if len(x) > 1 else None)\n",
    "\n",
    "# 添加 \"match\" 列，检查两个 code 是否相同\n",
    "comparison_df[\"match\"] = comparison_df.apply(lambda row: row[\"code_1\"] == row[\"code_2\"], axis=1)\n",
    "\n",
    "# 删除原来的 code 列\n",
    "comparison_df = comparison_df.drop(columns=[\"code\"])\n",
    "\n",
    "# 只保存前 200 行\n",
    "comparison_df.head(200).to_csv(\"Duplicate_Code_Comparison.csv\", index=False)\n",
    "\n",
    "print(\"Saved first 200 duplicate code comparisons as 'Duplicate_Code_Comparison.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 读取 Duplicate_Rows.csv\n",
    "duplicates_df = pd.read_csv(\"Duplicate_Rows.csv\")\n",
    "\n",
    "# 按 filename 排序\n",
    "duplicates_sorted = duplicates_df.sort_values(by=\"filename\")\n",
    "\n",
    "# 打印前 100 行\n",
    "print(duplicates_sorted.head(100))\n",
    "\n",
    "duplicates_sorted.head(100).to_csv(\"Top100_Duplicate_Rows.csv\", index=False)\n",
    "print(\"Saved first 100 duplicate rows as 'Top100_Duplicate_Rows.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
