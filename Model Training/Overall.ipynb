{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f05fb445-d34f-4381-83f5-1f7158833ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import joblib\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定義模型類別 (保持與原代碼一致)\n",
    "class VulnScreener(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VulnScreener, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class VulnAnalyzer(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(VulnAnalyzer, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(1, 256, kernel_size=1),\n",
    "            nn.AvgPool1d(kernel_size=8, stride=8)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256 * 96, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 9),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, p_s):\n",
    "        if p_s.dim() == 1:\n",
    "            p_s = p_s.unsqueeze(1)\n",
    "        x = torch.cat((x, p_s), dim=1).unsqueeze(1)\n",
    "        residual = self.residual(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        if residual.size(2) != x.size(2):\n",
    "            residual = nn.functional.interpolate(residual, size=x.size(2), mode='nearest')\n",
    "        x = x + residual\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x, None\n",
    "\n",
    "class VulnValidator:\n",
    "    def __init__(self, model_path='../codebert/vuln_validator_model.pkl', \n",
    "                 scaler_path='../codebert/scaler.pkl', \n",
    "                 pca_path='../codebert/pca.pkl'):\n",
    "        # 加載預訓練的隨機森林模型\n",
    "        self.rf = joblib.load(model_path)\n",
    "        # 加載預訓練的Scaler和PCA\n",
    "        self.scaler = joblib.load(scaler_path)\n",
    "        self.pca = joblib.load(pca_path)\n",
    "        self.feature_names = None  # 將在加載時設置\n",
    "    \n",
    "    def prepare_features(self, X, p_a, p_s, fit=False):\n",
    "        X = X.cpu().detach().numpy() if torch.is_tensor(X) else X\n",
    "        p_a = p_a.cpu().detach().numpy() if torch.is_tensor(p_a) else p_a\n",
    "        p_s = p_s.cpu().detach().numpy() if torch.is_tensor(p_s) else p_s\n",
    "        \n",
    "        # PCA 和統計特徵\n",
    "        if fit:\n",
    "            X_pca = self.pca.fit_transform(X)\n",
    "        else:\n",
    "            X_pca = self.pca.transform(X)\n",
    "        \n",
    "        stats = np.hstack([\n",
    "            X.mean(axis=1, keepdims=True),\n",
    "            X.var(axis=1, keepdims=True),\n",
    "            X.max(axis=1, keepdims=True),\n",
    "            X.min(axis=1, keepdims=True)\n",
    "        ])\n",
    "        interaction = p_a * p_s\n",
    "        \n",
    "        # 組合特徵\n",
    "        features = np.hstack([X_pca, p_a, p_s, stats, interaction])\n",
    "        if fit:\n",
    "            features = self.scaler.fit_transform(features)\n",
    "        else:\n",
    "            features = self.scaler.transform(features)\n",
    "        \n",
    "        # 設置特徵名稱（僅在fit時）\n",
    "        if fit:\n",
    "            num_pca = self.pca.n_components\n",
    "            num_p_a = p_a.shape[1]\n",
    "            num_p_s = p_s.shape[1]\n",
    "            self.feature_names = (\n",
    "                [f\"PCA_{i}\" for i in range(num_pca)] +\n",
    "                [f\"Analyzer_Prob_{i}\" for i in range(num_p_a)] +\n",
    "                [f\"Screener_Prob_{i}\" for i in range(num_p_s)] +\n",
    "                [\"Mean\", \"Variance\", \"Max\", \"Min\"] +\n",
    "                [f\"Interaction_{i}\" for i in range(num_p_a)]\n",
    "            )\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def fit(self, X, p_a, p_s, y_train):\n",
    "        features = self.prepare_features(X, p_a, p_s, fit=True)\n",
    "        y_train = y_train.cpu().detach().numpy() if torch.is_tensor(y_train) else y_train\n",
    "        self.rf.fit(features, y_train)\n",
    "    \n",
    "    def predict(self, X, p_a, p_s):\n",
    "        features = self.prepare_features(X, p_a, p_s, fit=False)\n",
    "        p_v = self.rf.predict_proba(features)\n",
    "        return np.stack([prob[:, 1] for prob in p_v], axis=1)\n",
    "    \n",
    "    def generate_validation_report(self, p_f, p_v, threshold=0.05):\n",
    "        report = {\"anomalies\": [], \"corrections\": [], \"top_features\": {}}\n",
    "        p_f = p_f.cpu().detach().numpy() if torch.is_tensor(p_f) else p_f\n",
    "        p_v = p_v.cpu().detach().numpy() if torch.is_tensor(p_v) else p_v\n",
    "        \n",
    "        # 檢測異常和修正\n",
    "        for j in range(p_f.shape[1]):\n",
    "            diff = np.abs(p_f[0, j] - p_v[0, j])\n",
    "            if diff > threshold:\n",
    "                if p_f[0, j] > 0.5 and p_v[0, j] < 0.5:\n",
    "                    report[\"anomalies\"].append(f\"Vuln {j}\")\n",
    "                elif p_f[0, j] < 0.5 and p_v[0, j] > 0.5:\n",
    "                    report[\"corrections\"].append(f\"Vuln {j}\")\n",
    "        \n",
    "        # 計算特徵重要性\n",
    "        try:\n",
    "            if hasattr(self.rf, \"estimators_\") and self.feature_names:\n",
    "                avg_importances = np.mean([est.feature_importances_ for est in self.rf.estimators_], axis=0)\n",
    "                top_5_idx = np.argsort(avg_importances)[-5:][::-1]\n",
    "                report[\"top_features\"] = {self.feature_names[idx]: float(avg_importances[idx]) for idx in top_5_idx}\n",
    "            else:\n",
    "                print(\"Warning: Feature importances not available.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating feature importances: {e}\")\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee693f7-3b5d-4639-8231-e47d33f1fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeBERT 特徵提取函數\n",
    "def extract_codebert_features(code_snippet, tokenizer, model, max_length=512):\n",
    "    inputs = tokenizer(code_snippet, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # 確保輸入在同一設備\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "def extract_codebert_features_long_code(code_snippet, tokenizer, model, max_length=512, stride=1024):\n",
    "    tokens = tokenizer.tokenize(code_snippet)\n",
    "    if len(tokens) <= max_length - 2:\n",
    "        return extract_codebert_features(code_snippet, tokenizer, model, max_length)\n",
    "\n",
    "    features = []\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        segment = tokens[i:i + max_length - 2]\n",
    "        if len(segment) > 0:\n",
    "            segment_code = tokenizer.convert_tokens_to_string(segment)\n",
    "            feat = extract_codebert_features(segment_code, tokenizer, model, max_length)\n",
    "            features.append(feat)\n",
    "    return np.max(features, axis=0) if features else np.zeros(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fedc25bb-6ae6-4575-8b72-ce9aaa5c4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢測單一智能合約\n",
    "def detect_smart_contract(code_snippet, screener, analyzer, validator, tokenizer, codebert_model, device, max_length=512, stride=1024):\n",
    "    \"\"\"\n",
    "    檢測單一智能合約的漏洞，從原始代碼開始\n",
    "    code_snippet: str - 原始智能合約代碼\n",
    "    \"\"\"\n",
    "    # 提取 CodeBERT 特徵\n",
    "    try:\n",
    "        contract_embedding = extract_codebert_features_long_code(code_snippet, tokenizer, codebert_model, max_length, stride)\n",
    "        contract_embedding = torch.tensor(contract_embedding, dtype=torch.float32).unsqueeze(0)  # [1, 768]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting CodeBERT features: {e}\")\n",
    "        contract_embedding = torch.zeros(1, 768, dtype=torch.float32)  # 默認值\n",
    "\n",
    "    contract_embedding = contract_embedding.to(device)\n",
    "    \n",
    "    # Step 1: VulnScreener\n",
    "    screener.eval()\n",
    "    with torch.no_grad():\n",
    "        screener_prob = screener(contract_embedding)  # [1, 1]\n",
    "    \n",
    "    # Step 2: VulnAnalyzer\n",
    "    analyzer.eval()\n",
    "    with torch.no_grad():\n",
    "        analyzer_prob, _ = analyzer(contract_embedding, screener_prob)  # [1, 9]\n",
    "    \n",
    "    # Step 3: VulnValidator\n",
    "    validator_prob = torch.from_numpy(validator.predict(\n",
    "        contract_embedding, analyzer_prob, screener_prob\n",
    "    )).to(device, dtype=torch.float32)  # [1, 9]\n",
    "    \n",
    "    # Step 4: 融合結果\n",
    "    final_prob = 0.5 * analyzer_prob + 0.5 * validator_prob  # [1, 9]\n",
    "    predictions = (final_prob > 0.5).int().cpu().numpy()[0]  # [9]\n",
    "    \n",
    "    # Step 5: 生成驗證報告\n",
    "    validation_report = validator.generate_validation_report(final_prob, validator_prob)\n",
    "    \n",
    "    # 漏洞名稱映射\n",
    "    vuln_names = [\n",
    "        \"No Vulnerability\", \"block number dependency (BN)\", \"dangerous delegatecall (DE)\",\n",
    "        \"ether frozen (EF)\", \"ether strict equality (SE)\", \"integer overflow (OF)\",\n",
    "        \"reentrancy (RE)\", \"timestamp dependency (TP)\", \"unchecked external call (UC)\"\n",
    "    ]\n",
    "    \n",
    "    # 生成最終報告\n",
    "    report = {\n",
    "        \"vulnerabilities_detected\": [vuln_names[i] for i, pred in enumerate(predictions) if pred == 1],\n",
    "        \"probability_scores\": {vuln_names[i]: float(final_prob[0, i]) for i in range(9)},\n",
    "        \"screener_vulnerability_score\": float(screener_prob[0, 0]),\n",
    "        \"anomalies\": [vuln_names[int(v.split()[1])] for v in validation_report[\"anomalies\"]],\n",
    "        \"corrections\": [vuln_names[int(v.split()[1])] for v in validation_report[\"corrections\"]],\n",
    "        \"top_features\": validation_report[\"top_features\"]\n",
    "    }\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f114e4-e63a-4ff3-9f0b-584580b0f816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Processing sample code snippet (index 3207):\n",
      "library SafeMathLib{ function mul(uint256 a, uint256 b) internal pure returns (uint256) { uint256 c = a * b; assert(a == 0 || c / a == b); return c; } function div(uint256 a, uint256 b) internal pure ...\n",
      "True Vulnerabilities from dataset: ['dangerous delegatecall (DE)', 'ether frozen (EF)']\n",
      "Warning: Feature importances not available.\n",
      "\n",
      "Smart Contract Vulnerability Detection Report:\n",
      "Overall Vulnerability Score: 8.3231\n",
      "Detected Vulnerabilities (Predicted): ['dangerous delegatecall (DE)', 'ether frozen (EF)']\n",
      "True Vulnerabilities (Dataset): ['dangerous delegatecall (DE)', 'ether frozen (EF)']\n",
      "Anomalies (Analyzer overconfident): []\n",
      "Corrections (Validator adjustments): []\n",
      "Top 5 Important Features: {}\n",
      "\n",
      "Detailed Probability Scores:\n",
      "No Vulnerability: 0.0536\n",
      "block number dependency (BN): 0.1010\n",
      "dangerous delegatecall (DE): 0.7916\n",
      "ether frozen (EF): 0.7224\n",
      "ether strict equality (SE): 0.1277\n",
      "integer overflow (OF): 0.2133\n",
      "reentrancy (RE): 0.2163\n",
      "timestamp dependency (TP): 0.2056\n",
      "unchecked external call (UC): 0.2138\n"
     ]
    }
   ],
   "source": [
    "# 主函數\n",
    "def main():\n",
    "    # 設置設備\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 初始化 CodeBERT\n",
    "    tokenizer_codebert = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "    model_codebert = RobertaModel.from_pretrained('microsoft/codebert-base').to(device)\n",
    "    \n",
    "    # 初始化並加載檢測模型\n",
    "    screener = VulnScreener().to(device)\n",
    "    analyzer = VulnAnalyzer().to(device)\n",
    "    validator = VulnValidator(\n",
    "        model_path='../codebert/vuln_validator_model.pkl',\n",
    "        scaler_path='../codebert/scaler.pkl',\n",
    "        pca_path='../codebert/pca.pkl'\n",
    "    )\n",
    "    \n",
    "    # 加載預訓練權重\n",
    "    screener = torch.load('../codebert/vuln_screener_model.pth', weights_only=False).to(device)\n",
    "    analyzer = torch.load('../codebert/vuln_analyzer_model.pth', weights_only=False).to(device)\n",
    "    \n",
    "    # 讀取數據集\n",
    "    df = pd.read_csv(\"../combined_dataset_cleaned.csv\")\n",
    "    \n",
    "    # 示例：檢測第一筆智能合約代碼\n",
    "    sample_idx = 3207  # 可修改為其他索引\n",
    "    sample_code = df['code'].iloc[sample_idx]\n",
    "    true_vulnerabilities_binary = eval(df['vulnerability_list'].iloc[sample_idx])  # 將字符串轉為列表\n",
    "    vuln_names = [\n",
    "        \"No Vulnerability\", \"block number dependency (BN)\", \"dangerous delegatecall (DE)\",\n",
    "        \"ether frozen (EF)\", \"ether strict equality (SE)\", \"integer overflow (OF)\",\n",
    "        \"reentrancy (RE)\", \"timestamp dependency (TP)\", \"unchecked external call (UC)\"\n",
    "    ]\n",
    "    true_vulnerabilities = [vuln_names[i] for i, val in enumerate(true_vulnerabilities_binary) if val == 1]\n",
    "    \n",
    "    print(f\"\\nProcessing sample code snippet (index {sample_idx}):\\n{sample_code[:200]}...\")\n",
    "    print(f\"True Vulnerabilities from dataset: {true_vulnerabilities}\")\n",
    "    \n",
    "    result = detect_smart_contract(\n",
    "        sample_code, screener, analyzer, validator, tokenizer_codebert, model_codebert, device\n",
    "    )\n",
    "    \n",
    "    # 輸出結果並比對\n",
    "    print(\"\\nSmart Contract Vulnerability Detection Report:\")\n",
    "    print(f\"Overall Vulnerability Score: {result['screener_vulnerability_score']:.4f}\")\n",
    "    print(\"Detected Vulnerabilities (Predicted):\", result['vulnerabilities_detected'])\n",
    "    print(\"True Vulnerabilities (Dataset):\", true_vulnerabilities)\n",
    "    print(\"Anomalies (Analyzer overconfident):\", result['anomalies'])\n",
    "    print(\"Corrections (Validator adjustments):\", result['corrections'])\n",
    "    print(\"Top 5 Important Features:\", result['top_features'])\n",
    "    print(\"\\nDetailed Probability Scores:\")\n",
    "    for vuln, prob in result['probability_scores'].items():\n",
    "        print(f\"{vuln}: {prob:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb1054-5d63-4bc2-8c69-941876c145c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
