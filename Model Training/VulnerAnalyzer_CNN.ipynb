{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107c4e0e-69b2-4351-8e25-0293bd6ef7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rita/Documents/9309_ML/Smart_Contract_Vulnerabilities_Project/Model Training\n",
      "Training on device: mps\n",
      "X_train shape: torch.Size([4294, 768])\n",
      "y_train shape: torch.Size([4294, 9])\n",
      "X_test shape: torch.Size([1074, 768])\n",
      "y_test shape: torch.Size([1074, 9])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, confusion_matrix, precision_score, f1_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# Check for MPS availability (Apple Silicon GPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(\"../CodeT5/train_data.csv\")\n",
    "df_test = pd.read_csv(\"../CodeT5/test_data.csv\")\n",
    "\n",
    "X_train = torch.load(\"../CodeT5/X_train.pt\", weights_only=False)\n",
    "X_test = torch.load(\"../CodeT5/X_test.pt\", weights_only=False)\n",
    "\n",
    "# Process vulnerability_list (9 dimensions)\n",
    "def process_vulnerability_list(vuln_list_series, num_classes=9):\n",
    "    vuln_lists = vuln_list_series.apply(ast.literal_eval)\n",
    "    y_binary = np.array([np.array(vuln) for vuln in vuln_lists], dtype=np.float32)\n",
    "    if y_binary.shape[1] != num_classes:\n",
    "        raise ValueError(f\"Expected {num_classes} dimensions, got {y_binary.shape[1]}\")\n",
    "    return torch.tensor(y_binary, dtype=torch.float32)\n",
    "\n",
    "y_train = process_vulnerability_list(df_train['vulnerability_list'], num_classes=9)\n",
    "y_test = process_vulnerability_list(df_test['vulnerability_list'], num_classes=9)\n",
    "\n",
    "torch.save(y_train, \"../CodeT5/y_train.pt\")\n",
    "torch.save(y_test, \"../CodeT5/y_test.pt\")\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9ee5e0-56f1-4a50-84d0-a3b3eec8663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VulnScreener and get probabilities\n",
    "class VulnScreener(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VulnScreener, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # Input layer to Hidden Layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),  # Hidden Layer 1 to Hidden Layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),    # Hidden Layer 2 to Output Layer\n",
    "            nn.Sigmoid()          # Probability output\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)  # Forward pass through the network\n",
    "screener = torch.load('../CodeT5/vuln_screener_model.pth', weights_only=False)\n",
    "screener.eval()\n",
    "with torch.no_grad():\n",
    "    train_prob = screener(X_train)\n",
    "    test_prob = screener(X_test)\n",
    "\n",
    "# Enhanced oversampling with noise injection\n",
    "def oversample_rare_classes(X, p_s, y, class_indices=[2, 3, 7], multiplier=5, noise_level=0.01):\n",
    "    mask = torch.any(y[:, class_indices] == 1, dim=1)\n",
    "    X_rare = X[mask]\n",
    "    p_s_rare = p_s[mask]\n",
    "    y_rare = y[mask]\n",
    "    for _ in range(multiplier):\n",
    "        noise = torch.randn_like(X_rare) * noise_level\n",
    "        X = torch.cat([X, X_rare + noise], dim=0)\n",
    "        p_s = torch.cat([p_s, p_s_rare], dim=0)\n",
    "        y = torch.cat([y, y_rare], dim=0)\n",
    "    return X, p_s, y\n",
    "\n",
    "X_train, train_prob, y_train = oversample_rare_classes(X_train, train_prob, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5795912-9a3c-474d-95e0-685c032dee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Focal Loss\n",
    "class WeightedFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2):\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha if alpha is not None else torch.ones(9).to(device)  # Default equal weights\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCELoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "# Enhanced VulnAnalyzer with corrected residual connection\n",
    "class VulnAnalyzer(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(VulnAnalyzer, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)  # 769 -> 384\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)  # 384 -> 192\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)  # 192 -> 96\n",
    "        )\n",
    "        # Adjust residual path to match output size of conv3 (256 channels, 96 length)\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(1, 256, kernel_size=1),\n",
    "            nn.AvgPool1d(kernel_size=8, stride=8)  # Downsample 769 to ~96\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256 * 96, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 9),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, p_s):\n",
    "        if p_s.dim() == 1:\n",
    "            p_s = p_s.unsqueeze(1)\n",
    "        x = torch.cat((x, p_s), dim=1).unsqueeze(1)  # [batch_size, 1, 769]\n",
    "        residual = self.residual(x)  # [batch_size, 256, 96]\n",
    "        x = self.conv1(x)  # [batch_size, 64, 384]\n",
    "        x = self.conv2(x)  # [batch_size, 128, 192]\n",
    "        x = self.conv3(x)  # [batch_size, 256, 96]\n",
    "        # Ensure residual matches xâ€™s size\n",
    "        if residual.size(2) != x.size(2):\n",
    "            residual = nn.functional.interpolate(residual, size=x.size(2), mode='nearest')\n",
    "        x = x + residual  # Residual connection\n",
    "        x = x.view(x.size(0), -1)  # [batch_size, 256 * 96]\n",
    "        x = self.fc_layers(x)\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa06c61d-b7b6-4d43-bee2-d3c9674e3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with early stopping\n",
    "def train_vuln_analyzer(X_train, p_s_train, y_train, threshold=0.7, epochs=200, lr=0.0001, \n",
    "                       dropout_rate=0.2, weight_decay=0.001, validator_feedback=None, patience=20):\n",
    "    model = VulnAnalyzer(dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    # Compute class weights based on inverse frequency\n",
    "    class_counts = y_train.sum(dim=0)\n",
    "    alpha = 1.0 / (class_counts + 1e-6)\n",
    "    alpha = alpha / alpha.sum() * 9  # Normalize to sum to 9\n",
    "    criterion = WeightedFocalLoss(alpha=alpha.to(device), gamma=2)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    X_train, p_s_train, y_train = X_train.to(device), p_s_train.to(device), y_train.to(device)\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mask = p_s_train > threshold\n",
    "        X_train_filtered = X_train[mask.squeeze()]\n",
    "        p_s_train_filtered = p_s_train[mask]\n",
    "        y_train_filtered = y_train[mask.squeeze()]\n",
    "        \n",
    "        if len(X_train_filtered) == 0:\n",
    "            threshold = max(0.1, threshold - 0.1)\n",
    "            mask = p_s_train > threshold\n",
    "            X_train_filtered = X_train[mask.squeeze()]\n",
    "            p_s_train_filtered = p_s_train[mask]\n",
    "            y_train_filtered = y_train[mask.squeeze()]\n",
    "        \n",
    "        p_a, _ = model(X_train_filtered, p_s_train_filtered)\n",
    "        loss = criterion(p_a, y_train_filtered)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred_binary = (p_a > 0.5).float()\n",
    "            train_f1 = f1_score(y_train_filtered.cpu(), y_pred_binary.cpu(), average='micro')\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if validator_feedback is not None and epoch % 10 == 0 and epoch > 0:\n",
    "            with torch.no_grad():\n",
    "                for vuln_idx, correction_factor in validator_feedback.items():\n",
    "                    if vuln_idx < 9 and correction_factor > 0:\n",
    "                        model.fc_layers[-2].weight[vuln_idx] *= (1 + correction_factor * 0.1)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Train F1: {train_f1:.4f}, Threshold: {threshold:.3f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if train_f1 > best_f1:\n",
    "            best_f1 = train_f1\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384556e7-c653-4ae4-981b-5a592bc1687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced testing function\n",
    "def test_vuln_analyzer(model, X_test, p_s_test, y_test, detailed_report=True):\n",
    "    model.eval()\n",
    "    X_test, p_s_test, y_test = X_test.to(device), p_s_test.to(device), y_test.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        p_a, attn_weights = model(X_test, p_s_test)\n",
    "        y_pred_proba = p_a.cpu().numpy()\n",
    "        y_true = y_test.cpu().numpy()\n",
    "    \n",
    "    metrics = {}\n",
    "    n_classes = y_true.shape[1]\n",
    "    optimal_thresholds = []\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_pred_proba[:, i])\n",
    "        f1_scores = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "        optimal_idx = np.argmax(f1_scores)\n",
    "        optimal_thresholds.append(thresholds[optimal_idx])\n",
    "    \n",
    "    y_pred_binary = np.zeros_like(y_pred_proba)\n",
    "    for i in range(n_classes):\n",
    "        y_pred_binary[:, i] = (y_pred_proba[:, i] >= optimal_thresholds[i]).astype(int)\n",
    "    \n",
    "    metrics['avg_precision'] = precision_score(y_true, y_pred_binary, average='micro')\n",
    "    metrics['avg_recall'] = recall_score(y_true, y_pred_binary, average='micro')\n",
    "    metrics['avg_f1'] = f1_score(y_true, y_pred_binary, average='micro')\n",
    "    metrics['avg_auc'] = roc_auc_score(y_true, y_pred_proba, average='micro')\n",
    "    metrics['optimal_thresholds'] = optimal_thresholds\n",
    "    \n",
    "    class_metrics = {\n",
    "        'precision': precision_score(y_true, y_pred_binary, average=None, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred_binary, average=None),\n",
    "        'f1': f1_score(y_true, y_pred_binary, average=None),\n",
    "        'auc': [roc_auc_score(y_true[:, i], y_pred_proba[:, i]) for i in range(n_classes)]\n",
    "    }\n",
    "    metrics['class_metrics'] = class_metrics\n",
    "    \n",
    "    print(\"\\n=== VulnAnalyzer Test Results (Optimal Thresholds) ===\")\n",
    "    print(f\"Test Samples: {len(X_test)}\")\n",
    "    print(f\"Average Precision: {metrics['avg_precision']:.4f}\")\n",
    "    print(f\"Average Recall: {metrics['avg_recall']:.4f}\")\n",
    "    print(f\"Average F1-Score: {metrics['avg_f1']:.4f}\")\n",
    "    print(f\"Average AUC: {metrics['avg_auc']:.4f}\")\n",
    "    \n",
    "    if detailed_report:\n",
    "        print(\"\\nPer-Class Metrics with Optimal Thresholds:\")\n",
    "        for i in range(n_classes):\n",
    "            print(f\"\\nVulnerability {i} (Threshold: {optimal_thresholds[i]:.3f}):\")\n",
    "            print(f\"Precision: {class_metrics['precision'][i]:.4f}\")\n",
    "            print(f\"Recall: {class_metrics['recall'][i]:.4f}\")\n",
    "            print(f\"F1-Score: {class_metrics['f1'][i]:.4f}\")\n",
    "            print(f\"AUC: {class_metrics['auc'][i]:.4f}\")\n",
    "    \n",
    "    return metrics, y_pred_proba, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c627c3-8e44-4dd4-b047-f43341839365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.1940, Train F1: 0.2786, Threshold: 0.700\n",
      "Epoch 10, Loss: 0.1093, Train F1: 0.4873, Threshold: 0.700\n",
      "Epoch 20, Loss: 0.0762, Train F1: 0.6329, Threshold: 0.700\n",
      "Epoch 30, Loss: 0.0597, Train F1: 0.7003, Threshold: 0.700\n",
      "Epoch 40, Loss: 0.0486, Train F1: 0.7586, Threshold: 0.700\n",
      "Epoch 50, Loss: 0.0410, Train F1: 0.8070, Threshold: 0.700\n",
      "Epoch 60, Loss: 0.0349, Train F1: 0.8438, Threshold: 0.700\n",
      "Epoch 70, Loss: 0.0305, Train F1: 0.8699, Threshold: 0.700\n",
      "Epoch 80, Loss: 0.0265, Train F1: 0.8919, Threshold: 0.700\n",
      "Epoch 90, Loss: 0.0234, Train F1: 0.9113, Threshold: 0.700\n",
      "Epoch 100, Loss: 0.0213, Train F1: 0.9152, Threshold: 0.700\n",
      "Epoch 110, Loss: 0.0195, Train F1: 0.9302, Threshold: 0.700\n",
      "Epoch 120, Loss: 0.0184, Train F1: 0.9343, Threshold: 0.700\n",
      "Epoch 130, Loss: 0.0172, Train F1: 0.9381, Threshold: 0.700\n",
      "Epoch 140, Loss: 0.0166, Train F1: 0.9403, Threshold: 0.700\n",
      "Epoch 150, Loss: 0.0162, Train F1: 0.9422, Threshold: 0.700\n",
      "Epoch 160, Loss: 0.0156, Train F1: 0.9430, Threshold: 0.700\n",
      "Epoch 170, Loss: 0.0152, Train F1: 0.9463, Threshold: 0.700\n",
      "Early stopping at epoch 171\n",
      "\n",
      "=== VulnAnalyzer Test Results (Optimal Thresholds) ===\n",
      "Test Samples: 1074\n",
      "Average Precision: 0.7772\n",
      "Average Recall: 0.7547\n",
      "Average F1-Score: 0.7658\n",
      "Average AUC: 0.8421\n",
      "\n",
      "Per-Class Metrics with Optimal Thresholds:\n",
      "\n",
      "Vulnerability 0 (Threshold: 0.216):\n",
      "Precision: 0.7997\n",
      "Recall: 0.9061\n",
      "F1-Score: 0.8496\n",
      "AUC: 0.8681\n",
      "\n",
      "Vulnerability 1 (Threshold: 0.624):\n",
      "Precision: 0.8214\n",
      "Recall: 0.6216\n",
      "F1-Score: 0.7077\n",
      "AUC: 0.9126\n",
      "\n",
      "Vulnerability 2 (Threshold: 0.825):\n",
      "Precision: 0.6000\n",
      "Recall: 0.1765\n",
      "F1-Score: 0.2727\n",
      "AUC: 0.8444\n",
      "\n",
      "Vulnerability 3 (Threshold: 0.780):\n",
      "Precision: 0.6000\n",
      "Recall: 0.1765\n",
      "F1-Score: 0.2727\n",
      "AUC: 0.8608\n",
      "\n",
      "Vulnerability 4 (Threshold: 0.664):\n",
      "Precision: 0.6250\n",
      "Recall: 0.4237\n",
      "F1-Score: 0.5051\n",
      "AUC: 0.7971\n",
      "\n",
      "Vulnerability 5 (Threshold: 0.661):\n",
      "Precision: 0.5849\n",
      "Recall: 0.5688\n",
      "F1-Score: 0.5767\n",
      "AUC: 0.8959\n",
      "\n",
      "Vulnerability 6 (Threshold: 0.537):\n",
      "Precision: 0.8415\n",
      "Recall: 0.7541\n",
      "F1-Score: 0.7954\n",
      "AUC: 0.9142\n",
      "\n",
      "Vulnerability 7 (Threshold: 0.546):\n",
      "Precision: 0.5652\n",
      "Recall: 0.5000\n",
      "F1-Score: 0.5306\n",
      "AUC: 0.8681\n",
      "\n",
      "Vulnerability 8 (Threshold: 0.562):\n",
      "Precision: 0.8408\n",
      "Recall: 0.7213\n",
      "F1-Score: 0.7765\n",
      "AUC: 0.9254\n",
      "\n",
      "Sample Probabilities (first 5): [[0.35380605 0.18060987 0.08668131 0.160755   0.16387719 0.11999106\n",
      "  0.5785455  0.24295086 0.61113214]\n",
      " [0.19979586 0.2813511  0.09676918 0.17496955 0.24861671 0.57233447\n",
      "  0.30357745 0.3250008  0.29967168]\n",
      " [0.24009296 0.14755757 0.09344167 0.18237402 0.1941096  0.3763042\n",
      "  0.53748626 0.24668951 0.5775239 ]\n",
      " [0.31271622 0.2932028  0.66375864 0.6506463  0.17427444 0.13641387\n",
      "  0.32868922 0.15535149 0.30132598]\n",
      " [0.14480793 0.23506004 0.07002158 0.14298312 0.2317567  0.2871402\n",
      "  0.2763403  0.61608315 0.2606818 ]]\n",
      "Attention Weights Shape: None (CNN)\n"
     ]
    }
   ],
   "source": [
    "# Final training and testing\n",
    "validator_feedback = {0: 0.5, 2: 0.3}\n",
    "final_model = train_vuln_analyzer(\n",
    "    X_train, train_prob, y_train,\n",
    "    threshold=0.7, epochs=200, lr=0.0001, dropout_rate=0.2, weight_decay=0.001,\n",
    "    validator_feedback=validator_feedback\n",
    ")\n",
    "metrics, probabilities, attn_weights = test_vuln_analyzer(final_model, X_test, test_prob, y_test)\n",
    "\n",
    "print(\"\\nSample Probabilities (first 5):\", probabilities[:5])\n",
    "print(\"Attention Weights Shape:\", attn_weights.shape if attn_weights is not None else \"None (CNN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0745a073-cc56-4590-adfd-36a5d5881b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model, '../CodeT5/vuln_analyzer_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
