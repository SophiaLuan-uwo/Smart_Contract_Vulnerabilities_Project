{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "925919eb-c179-4181-8c16-6ea56137aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rita/Documents/9309_ML/Smart_Contract_Vulnerabilities_Project/Model Training\n",
      "Training on device: mps\n",
      "X_train shape: torch.Size([4294, 768])\n",
      "y_train shape: torch.Size([4294, 9])\n",
      "X_test shape: torch.Size([1074, 768])\n",
      "y_test shape: torch.Size([1074, 9])\n",
      "screener_train_prob: torch.Size([4294, 1])\n",
      "screener_test_prob: torch.Size([1074, 1])\n",
      "analyzer_train_prob: torch.Size([4294, 9])\n",
      "analyzer_test_prob: torch.Size([1074, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# 轉換 X_train / X_test 為 NumPy 並與 Screener 預測機率拼接\\nX_train_np = X_train.cpu().numpy()\\nX_test_np = X_test.cpu().numpy()\\n\\nX_train_combined = np.hstack([X_train_np, screener_train_prob.cpu()])\\nX_test_combined = np.hstack([X_test_np, screener_test_prob.cpu()])\\n\\n# XGBoost 進行預測\\ny_pred_train = analyzer.predict(X_train_combined)\\ny_pred_test = analyzer.predict(X_test_combined)\\n\\n# 取得機率\\nanalyzer_train_prob = np.array([est.predict_proba(X_train_combined)[:, 1] for est in analyzer.estimators_]).T\\nanalyzer_test_prob = np.array([est.predict_proba(X_test_combined)[:, 1] for est in analyzer.estimators_]).T'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import joblib\n",
    "import ast\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import skew, kurtosis\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score, precision_recall_curve, confusion_matrix, precision_score, f1_score, recall_score\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# Check for MPS availability (Apple Silicon GPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "X_train = torch.load(\"../codebert/X_train.pt\", weights_only=False).to(device)\n",
    "X_test = torch.load(\"../codebert/X_test.pt\", weights_only=False).to(device)\n",
    "y_train = torch.load(\"../codebert/y_train.pt\", weights_only=False).to(device)\n",
    "y_test = torch.load(\"../codebert/y_test.pt\", weights_only=False).to(device)\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Load VulnScreener and get probabilities\n",
    "class VulnScreener(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VulnScreener, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # Input layer to Hidden Layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),  # Hidden Layer 1 to Hidden Layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),    # Hidden Layer 2 to Output Layer\n",
    "            nn.Sigmoid()          # Probability output\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)  # Forward pass through the network\n",
    "\n",
    "class VulnAnalyzer(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(VulnAnalyzer, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)  # 769 -> 384\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)  # 384 -> 192\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)  # 192 -> 96\n",
    "        )\n",
    "        # Adjust residual path to match output size of conv3 (256 channels, 96 length)\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(1, 256, kernel_size=1),\n",
    "            nn.AvgPool1d(kernel_size=8, stride=8)  # Downsample 769 to ~96\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256 * 96, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 9),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, p_s):\n",
    "        if p_s.dim() == 1:\n",
    "            p_s = p_s.unsqueeze(1)\n",
    "        x = torch.cat((x, p_s), dim=1).unsqueeze(1)  # [batch_size, 1, 769]\n",
    "        residual = self.residual(x)  # [batch_size, 256, 96]\n",
    "        x = self.conv1(x)  # [batch_size, 64, 384]\n",
    "        x = self.conv2(x)  # [batch_size, 128, 192]\n",
    "        x = self.conv3(x)  # [batch_size, 256, 96]\n",
    "        # Ensure residual matches x’s size\n",
    "        if residual.size(2) != x.size(2):\n",
    "            residual = nn.functional.interpolate(residual, size=x.size(2), mode='nearest')\n",
    "        x = x + residual  # Residual connection\n",
    "        x = x.view(x.size(0), -1)  # [batch_size, 256 * 96]\n",
    "        x = self.fc_layers(x)\n",
    "        return x, None\n",
    "\n",
    "screener = VulnScreener().to(device)\n",
    "#analyzer = VulnAnalyzer().to(device)\n",
    "\n",
    "screener = torch.load('../codebert/vuln_screener_model.pth', weights_only=False).to(device)\n",
    "#analyzer = joblib.load('../codebert/vuln_analyzer_XGB_model.pkl')\n",
    "analyzer = torch.load('../codebert/vuln_analyzer_model.pth', weights_only=False).to(device)\n",
    "\n",
    "# 載入最佳 Thresholds\n",
    "#with open(\"../codebert/optimal_thresholds.json\", \"r\") as f:\n",
    "#    optimal_thresholds = json.load(f)\n",
    "\n",
    "# 確保 Thresholds 為 NumPy 陣列\n",
    "#thresholds = np.array(list(optimal_thresholds.values()))\n",
    "\n",
    "\n",
    "screener.eval()\n",
    "analyzer.eval()\n",
    "\n",
    "# 串接流程\n",
    "with torch.no_grad():\n",
    "    screener_train_prob = screener(X_train).to(device)\n",
    "    screener_test_prob = screener(X_test).to(device)\n",
    "    analyzer_train_prob,_ = analyzer(X_train, screener_train_prob)\n",
    "    analyzer_test_prob,_ = analyzer(X_test, screener_test_prob)\n",
    "    print(f\"screener_train_prob: {screener_train_prob.shape}\") \n",
    "    print(f\"screener_test_prob: {screener_test_prob.shape}\")\n",
    "    print(f\"analyzer_train_prob: {analyzer_train_prob.shape}\")\n",
    "    print(f\"analyzer_test_prob: {analyzer_test_prob.shape}\")\n",
    "\n",
    "'''# 轉換 X_train / X_test 為 NumPy 並與 Screener 預測機率拼接\n",
    "X_train_np = X_train.cpu().numpy()\n",
    "X_test_np = X_test.cpu().numpy()\n",
    "\n",
    "X_train_combined = np.hstack([X_train_np, screener_train_prob.cpu()])\n",
    "X_test_combined = np.hstack([X_test_np, screener_test_prob.cpu()])\n",
    "\n",
    "# XGBoost 進行預測\n",
    "y_pred_train = analyzer.predict(X_train_combined)\n",
    "y_pred_test = analyzer.predict(X_test_combined)\n",
    "\n",
    "# 取得機率\n",
    "analyzer_train_prob = np.array([est.predict_proba(X_train_combined)[:, 1] for est in analyzer.estimators_]).T\n",
    "analyzer_test_prob = np.array([est.predict_proba(X_test_combined)[:, 1] for est in analyzer.estimators_]).T'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c653e35-2c2f-4214-b44a-c689a7dbc984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VulnValidator without attention weights\n",
    "class VulnValidator:\n",
    "    def __init__(self, n_trees=500, max_depth=15, pca_components=50):\n",
    "        self.rf = MultiOutputClassifier(\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=n_trees,\n",
    "                max_depth=max_depth,\n",
    "                random_state=42,\n",
    "                class_weight=\"balanced_subsample\"  # Better imbalance handling\n",
    "            )\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=pca_components)\n",
    "        self.feature_importance = None\n",
    "        self.feature_names = None\n",
    "    \n",
    "    def prepare_features(self, X, p_a, p_s):\n",
    "        X = X.cpu().detach().numpy() if torch.is_tensor(X) else X\n",
    "        p_a = p_a.cpu().detach().numpy() if torch.is_tensor(p_a) else p_a\n",
    "        p_s = p_s.cpu().detach().numpy() if torch.is_tensor(p_s) else p_s\n",
    "        \n",
    "        # Use PCA on X to retain more information\n",
    "        #X_pca = self.pca.fit_transform(X)\n",
    "        \n",
    "        # Statistical features\n",
    "        stats = np.hstack([\n",
    "            X.mean(axis=1, keepdims=True),\n",
    "            X.var(axis=1, keepdims=True),\n",
    "            X.max(axis=1, keepdims=True),\n",
    "            X.min(axis=1, keepdims=True)\n",
    "        ])\n",
    "        \n",
    "        # Interaction terms\n",
    "        interaction = p_a * p_s\n",
    "        \n",
    "        # Combine features\n",
    "        features = np.hstack([X, p_a, p_s, stats, interaction])\n",
    "        features = self.scaler.fit_transform(features)\n",
    "        return features\n",
    "    \n",
    "    def fit(self, X, p_a, p_s, y_train):\n",
    "        features = self.prepare_features(X, p_a, p_s)\n",
    "        y_train = y_train.cpu().detach().numpy() if torch.is_tensor(y_train) else y_train\n",
    "        \n",
    "        # Fit the model\n",
    "        self.rf.fit(features, y_train)\n",
    "        \n",
    "        # Aggregate feature importances\n",
    "        self.feature_importance = np.mean([est.feature_importances_ for est in self.rf.estimators_], axis=0)\n",
    "        \n",
    "        # Feature names\n",
    "        num = X.shape[1]\n",
    "        num_p_a = p_a.shape[1]\n",
    "        num_p_s = p_s.shape[1]\n",
    "        self.feature_names = (\n",
    "            [f\"X_{i}\" for i in range(num)] +\n",
    "            [f\"Analyzer_Prob_{i}\" for i in range(num_p_a)] +\n",
    "            [f\"Screener_Prob_{i}\" for i in range(num_p_s)] +\n",
    "            [\"Mean\", \"Variance\", \"Max\", \"Min\"] +\n",
    "            [f\"Interaction_{i}\" for i in range(num_p_a)]\n",
    "        )\n",
    "    \n",
    "    def predict(self, X, p_a, p_s):\n",
    "        features = self.prepare_features(X, p_a, p_s)\n",
    "        p_v = self.rf.predict_proba(features)  # List of [n_samples, 2] arrays\n",
    "        p_v = np.stack([prob[:, 1] for prob in p_v], axis=1)  # [n_samples, 9]\n",
    "        return p_v\n",
    "    \n",
    "    def generate_validation_report(self, p_f, p_v, threshold=0.05, per_sample=False):\n",
    "        report = {\"anomalies\": [], \"corrections\": [], \"top_features\": {}}\n",
    "        if per_sample:\n",
    "            report = {i: {\"anomalies\": [], \"corrections\": [], \"top_features\": {}} for i in range(p_f.shape[0])}\n",
    "    \n",
    "        p_f = p_f.cpu().detach().numpy() if torch.is_tensor(p_f) else p_f\n",
    "        p_v = p_v.cpu().detach().numpy() if torch.is_tensor(p_v) else p_v\n",
    "    \n",
    "        # Ensure shapes match\n",
    "        assert p_f.shape == p_v.shape, f\"Shape mismatch: p_f {p_f.shape}, p_v {p_v.shape}\"\n",
    "    \n",
    "        for i in range(p_f.shape[0]):  # Iterate over samples\n",
    "            sample_anomalies = []\n",
    "            sample_corrections = []\n",
    "            for j in range(p_f.shape[1]):  # Iterate over vulnerabilities\n",
    "                diff = np.abs(p_f[i, j] - p_v[i, j])\n",
    "                #print(f\"p_f:{p_f[i,j]}\")\n",
    "                #print(f\"p_v:{p_v[i,j]}\")\n",
    "                #print(f\"diff:{diff}\")\n",
    "                if diff > threshold:\n",
    "                    if p_f[i, j] > 0.5 and p_v[i, j] < 0.5:\n",
    "                        sample_anomalies.append(f\"Vuln {j}\")\n",
    "                    elif p_f[i, j] < 0.5 and p_v[i, j] > 0.5:\n",
    "                        sample_corrections.append(f\"Vuln {j}\")\n",
    "        \n",
    "            if per_sample:\n",
    "                report[i][\"anomalies\"] = sample_anomalies\n",
    "                report[i][\"corrections\"] = sample_corrections\n",
    "            else:\n",
    "                report[\"anomalies\"].extend(sample_anomalies)\n",
    "                report[\"corrections\"].extend(sample_corrections)\n",
    "                \n",
    "        print(\"Calculating feature importances...\")\n",
    "        # Compute top features from all estimators\n",
    "        if hasattr(self.rf, \"estimators_\"):\n",
    "            print(f\"Number of estimators: {len(self.rf.estimators_)}\")\n",
    "            # Aggregate feature importances across all classifiers\n",
    "            avg_importances = np.mean([est.feature_importances_ for est in self.rf.estimators_], axis=0)\n",
    "            print(f\"Feature importances shape: {avg_importances.shape}\")\n",
    "            print(f\"Feature names length: {len(self.feature_names) if self.feature_names else 'None'}\")\n",
    "            top_5_idx = np.argsort(avg_importances)[-5:][::-1]\n",
    "            report[\"top_features\"] = {self.feature_names[idx]: float(avg_importances[idx]) for idx in top_5_idx}\n",
    "            if per_sample:\n",
    "                for i in range(p_f.shape[0]):\n",
    "                    report[i][\"top_features\"] = report[\"top_features\"]\n",
    "    \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc9136a-b957-48d7-8080-4346e23d823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_outputs(p_a, p_v):\n",
    "    p_v = p_v.to(p_a.device)\n",
    "    p_v = p_v.expand(-1, p_a.shape[1])\n",
    "    \n",
    "    p_f = 0.5 * p_a + 0.5 * p_v\n",
    "    \n",
    "    return p_f\n",
    "\n",
    "def evaluate_predictions(y_true, y_prob, threshold=0.5):\n",
    "    \"\"\"\n",
    "    評估預測結果，計算整體及各漏洞的 Precision, Recall, F1-score, AUC。\n",
    "\n",
    "    :param y_true: 真實標籤 (numpy array) - (N, num_vulns)\n",
    "    :param y_prob: 預測機率 (numpy array) - (N, num_vulns)\n",
    "    :param threshold: 判定為正類別的閾值，預設為 0.5\n",
    "    \"\"\"\n",
    "    if isinstance(y_prob, torch.Tensor):\n",
    "        y_prob = y_prob.cpu().numpy()  # 轉為 NumPy，確保在 CPU 上運行\n",
    "    \n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.cpu().numpy()  # 同樣轉換 y_true\n",
    "    \n",
    "    # 轉換為二元標籤\n",
    "    y_pred = (y_prob > threshold).astype(int)\n",
    "\n",
    "    # 計算 Overall 指標\n",
    "    overall_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    overall_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    overall_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    overall_auc = roc_auc_score(y_true, y_prob, average='macro') if len(set(y_true.flatten())) > 1 else None\n",
    "\n",
    "    print(f\"\\nOverall Precision: {overall_precision:.4f}\")\n",
    "    print(f\"Overall Recall: {overall_recall:.4f}\")\n",
    "    print(f\"Overall F1-Score: {overall_f1:.4f}\")\n",
    "    print(f\"Overall AUC: {overall_auc:.4f}\\n\")\n",
    "\n",
    "    # 計算各個漏洞的 Precision, Recall, F1-score, AUC\n",
    "    num_vulns = y_true.shape[1]\n",
    "    for i in range(num_vulns):\n",
    "        precision = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        recall = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        f1 = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "        auc = roc_auc_score(y_true[:, i], y_prob[:, i]) if len(set(y_true[:, i])) > 1 else None\n",
    "\n",
    "        print(f\"Vuln {i}: Precision={precision:.4f}, Recall={recall:.4f}, \"\n",
    "              f\"F1-Score={f1:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "# Validation\n",
    "def evaluate_results(p_f, y_true, analyzer_prob, validator_prob):\n",
    "    p_f = p_f.cpu().numpy()  # Convert to numpy for easier handling\n",
    "    y_true = y_true.cpu().numpy()  # Convert to numpy for comparison\n",
    "    \n",
    "    for i in range(p_f.shape[0]):  # 遍歷每一筆測試資料\n",
    "        print(f\"\\nEvaluating sample {i+1}/{p_f.shape[0]}:\")\n",
    "\n",
    "        # 這裡將單一資料的預測結果轉為二進制（0或1）\n",
    "        y_pred = (p_f[i, ] > 0.5).astype(int)\n",
    "        y_true_sample = y_true[i,]\n",
    "\n",
    "        # Print the evaluation metrics for the current sample\n",
    "        print(f\"Predicted: {y_pred}\")\n",
    "        print(f\"True: {y_true_sample}\")\n",
    "\n",
    "        # 生成和顯示該筆資料的報告\n",
    "        report = validator.generate_validation_report(p_f[i].reshape(1, -1), validator_prob[i].reshape(1, -1))\n",
    "        print(f\"Anomalies(Analyzer) for sample {i}: {report['anomalies']}\")\n",
    "        print(f\"Corrections(Validator) for sample {i}: {report['corrections']}\")\n",
    "\n",
    "        # 顯示該筆資料的前 5 個特徵\n",
    "        print(f\"Top 5 Features for sample {i}: {report['top_features']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a77d6a-8429-484f-8336-b0b972bdac5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'shpae'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m validator \u001b[38;5;241m=\u001b[39m VulnValidator()\n\u001b[0;32m----> 2\u001b[0m validator\u001b[38;5;241m.\u001b[39mfit(X_train, analyzer_train_prob, screener_train_prob, y_train)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get validator predictions\u001b[39;00m\n\u001b[1;32m      5\u001b[0m validator_test_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(validator\u001b[38;5;241m.\u001b[39mpredict(X_test, analyzer_test_prob, screener_test_prob))\n",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m, in \u001b[0;36mVulnValidator.fit\u001b[0;34m(self, X, p_a, p_s, y_train)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_importance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([est\u001b[38;5;241m.\u001b[39mfeature_importances_ \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrf\u001b[38;5;241m.\u001b[39mestimators_], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Feature names\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m num \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshpae[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     53\u001b[0m num_p_a \u001b[38;5;241m=\u001b[39m p_a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     54\u001b[0m num_p_s \u001b[38;5;241m=\u001b[39m p_s\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'shpae'"
     ]
    }
   ],
   "source": [
    "validator = VulnValidator()\n",
    "validator.fit(X_train, analyzer_train_prob, screener_train_prob, y_train)\n",
    "\n",
    "# Get validator predictions\n",
    "validator_test_prob = torch.from_numpy(validator.predict(X_test, analyzer_test_prob, screener_test_prob))\n",
    "validator_test_prob = validator_test_prob.clone().detach().to(device, dtype=torch.float32)\n",
    "evaluate_predictions(y_test, validator_test_prob)\n",
    "\n",
    "# Fuse outputs\n",
    "fuse_test = fuse_outputs(analyzer_test_prob, validator_test_prob)\n",
    "evaluate_predictions(y_test, fuse_test)\n",
    "\n",
    "evaluate_results(fuse_test, y_test, analyzer_test_prob, validator_test_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c4f88-5519-44b8-8528-c6545bbe4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(validator.rf, '../codebert/vuln_validator_model.pkl')\n",
    "joblib.dump(validator.scaler, '../codebert/scaler.pkl')\n",
    "joblib.dump(validator.pca, '../codebert/pca.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f128bf-b56d-4e9d-aaf5-75bafa696f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
