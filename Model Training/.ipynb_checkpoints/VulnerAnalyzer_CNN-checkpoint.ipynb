{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107c4e0e-69b2-4351-8e25-0293bd6ef7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rita/Documents/9309_ML/Smart_Contract_Vulnerabilities_Project/Model Training\n",
      "Training on device: mps\n",
      "X_train shape: torch.Size([4294, 768])\n",
      "y_train shape: torch.Size([4294, 9])\n",
      "X_test shape: torch.Size([1074, 768])\n",
      "y_test shape: torch.Size([1074, 9])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, confusion_matrix, precision_score, f1_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# Check for MPS availability (Apple Silicon GPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(\"../codebert/train_data.csv\")\n",
    "df_test = pd.read_csv(\"../codebert/test_data.csv\")\n",
    "\n",
    "X_train = torch.load(\"../codebert/X_train.pt\", weights_only=False)\n",
    "X_test = torch.load(\"../codebert/X_test.pt\", weights_only=False)\n",
    "\n",
    "# Process vulnerability_list (9 dimensions)\n",
    "def process_vulnerability_list(vuln_list_series, num_classes=9):\n",
    "    vuln_lists = vuln_list_series.apply(ast.literal_eval)\n",
    "    y_binary = np.array([np.array(vuln) for vuln in vuln_lists], dtype=np.float32)\n",
    "    if y_binary.shape[1] != num_classes:\n",
    "        raise ValueError(f\"Expected {num_classes} dimensions, got {y_binary.shape[1]}\")\n",
    "    return torch.tensor(y_binary, dtype=torch.float32)\n",
    "\n",
    "y_train = process_vulnerability_list(df_train['vulnerability_list'], num_classes=9)\n",
    "y_test = process_vulnerability_list(df_test['vulnerability_list'], num_classes=9)\n",
    "\n",
    "torch.save(y_train, \"../codebert/y_train.pt\")\n",
    "torch.save(y_test, \"../codebert/y_test.pt\")\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9ee5e0-56f1-4a50-84d0-a3b3eec8663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VulnScreener and get probabilities\n",
    "class VulnScreener(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VulnScreener, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # Input layer to Hidden Layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),  # Hidden Layer 1 to Hidden Layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),    # Hidden Layer 2 to Output Layer\n",
    "            nn.Sigmoid()          # Probability output\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)  # Forward pass through the network\n",
    "screener = torch.load('../codebert/vuln_screener_model.pth', weights_only=False)\n",
    "screener.eval()\n",
    "with torch.no_grad():\n",
    "    train_prob = screener(X_train)\n",
    "    test_prob = screener(X_test)\n",
    "\n",
    "# Enhanced oversampling with noise injection\n",
    "def oversample_rare_classes(X, p_s, y, class_indices=[2, 3, 7], multiplier=5, noise_level=0.01):\n",
    "    mask = torch.any(y[:, class_indices] == 1, dim=1)\n",
    "    X_rare = X[mask]\n",
    "    p_s_rare = p_s[mask]\n",
    "    y_rare = y[mask]\n",
    "    for _ in range(multiplier):\n",
    "        noise = torch.randn_like(X_rare) * noise_level\n",
    "        X = torch.cat([X, X_rare + noise], dim=0)\n",
    "        p_s = torch.cat([p_s, p_s_rare], dim=0)\n",
    "        y = torch.cat([y, y_rare], dim=0)\n",
    "    return X, p_s, y\n",
    "\n",
    "X_train, train_prob, y_train = oversample_rare_classes(X_train, train_prob, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5795912-9a3c-474d-95e0-685c032dee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Focal Loss\n",
    "class WeightedFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2):\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha if alpha is not None else torch.ones(9).to(device)  # Default equal weights\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCELoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "# Enhanced VulnAnalyzer with corrected residual connection\n",
    "class VulnAnalyzer(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(VulnAnalyzer, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)  # 769 -> 384\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)  # 384 -> 192\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)  # 192 -> 96\n",
    "        )\n",
    "        # Adjust residual path to match output size of conv3 (256 channels, 96 length)\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(1, 256, kernel_size=1),\n",
    "            nn.AvgPool1d(kernel_size=8, stride=8)  # Downsample 769 to ~96\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256 * 96, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 9),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, p_s):\n",
    "        if p_s.dim() == 1:\n",
    "            p_s = p_s.unsqueeze(1)\n",
    "        x = torch.cat((x, p_s), dim=1).unsqueeze(1)  # [batch_size, 1, 769]\n",
    "        residual = self.residual(x)  # [batch_size, 256, 96]\n",
    "        x = self.conv1(x)  # [batch_size, 64, 384]\n",
    "        x = self.conv2(x)  # [batch_size, 128, 192]\n",
    "        x = self.conv3(x)  # [batch_size, 256, 96]\n",
    "        # Ensure residual matches xâ€™s size\n",
    "        if residual.size(2) != x.size(2):\n",
    "            residual = nn.functional.interpolate(residual, size=x.size(2), mode='nearest')\n",
    "        x = x + residual  # Residual connection\n",
    "        x = x.view(x.size(0), -1)  # [batch_size, 256 * 96]\n",
    "        x = self.fc_layers(x)\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa06c61d-b7b6-4d43-bee2-d3c9674e3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with early stopping\n",
    "def train_vuln_analyzer(X_train, p_s_train, y_train, threshold=0.7, epochs=200, lr=0.0001, \n",
    "                       dropout_rate=0.2, weight_decay=0.001, validator_feedback=None, patience=20):\n",
    "    model = VulnAnalyzer(dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    # Compute class weights based on inverse frequency\n",
    "    class_counts = y_train.sum(dim=0)\n",
    "    alpha = 1.0 / (class_counts + 1e-6)\n",
    "    alpha = alpha / alpha.sum() * 9  # Normalize to sum to 9\n",
    "    criterion = WeightedFocalLoss(alpha=alpha.to(device), gamma=2)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    X_train, p_s_train, y_train = X_train.to(device), p_s_train.to(device), y_train.to(device)\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mask = p_s_train > threshold\n",
    "        X_train_filtered = X_train[mask.squeeze()]\n",
    "        p_s_train_filtered = p_s_train[mask]\n",
    "        y_train_filtered = y_train[mask.squeeze()]\n",
    "        \n",
    "        if len(X_train_filtered) == 0:\n",
    "            threshold = max(0.1, threshold - 0.1)\n",
    "            mask = p_s_train > threshold\n",
    "            X_train_filtered = X_train[mask.squeeze()]\n",
    "            p_s_train_filtered = p_s_train[mask]\n",
    "            y_train_filtered = y_train[mask.squeeze()]\n",
    "        \n",
    "        p_a, _ = model(X_train_filtered, p_s_train_filtered)\n",
    "        loss = criterion(p_a, y_train_filtered)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred_binary = (p_a > 0.5).float()\n",
    "            train_f1 = f1_score(y_train_filtered.cpu(), y_pred_binary.cpu(), average='micro')\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if validator_feedback is not None and epoch % 10 == 0 and epoch > 0:\n",
    "            with torch.no_grad():\n",
    "                for vuln_idx, correction_factor in validator_feedback.items():\n",
    "                    if vuln_idx < 9 and correction_factor > 0:\n",
    "                        model.fc_layers[-2].weight[vuln_idx] *= (1 + correction_factor * 0.1)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Train F1: {train_f1:.4f}, Threshold: {threshold:.3f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if train_f1 > best_f1:\n",
    "            best_f1 = train_f1\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384556e7-c653-4ae4-981b-5a592bc1687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced testing function\n",
    "def test_vuln_analyzer(model, X_test, p_s_test, y_test, detailed_report=True):\n",
    "    model.eval()\n",
    "    X_test, p_s_test, y_test = X_test.to(device), p_s_test.to(device), y_test.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        p_a, attn_weights = model(X_test, p_s_test)\n",
    "        y_pred_proba = p_a.cpu().numpy()\n",
    "        y_true = y_test.cpu().numpy()\n",
    "    \n",
    "    metrics = {}\n",
    "    n_classes = y_true.shape[1]\n",
    "    optimal_thresholds = []\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_pred_proba[:, i])\n",
    "        f1_scores = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "        optimal_idx = np.argmax(f1_scores)\n",
    "        optimal_thresholds.append(thresholds[optimal_idx])\n",
    "    \n",
    "    y_pred_binary = np.zeros_like(y_pred_proba)\n",
    "    for i in range(n_classes):\n",
    "        y_pred_binary[:, i] = (y_pred_proba[:, i] >= optimal_thresholds[i]).astype(int)\n",
    "    \n",
    "    metrics['avg_precision'] = precision_score(y_true, y_pred_binary, average='micro')\n",
    "    metrics['avg_recall'] = recall_score(y_true, y_pred_binary, average='micro')\n",
    "    metrics['avg_f1'] = f1_score(y_true, y_pred_binary, average='micro')\n",
    "    metrics['avg_auc'] = roc_auc_score(y_true, y_pred_proba, average='micro')\n",
    "    metrics['optimal_thresholds'] = optimal_thresholds\n",
    "    \n",
    "    class_metrics = {\n",
    "        'precision': precision_score(y_true, y_pred_binary, average=None, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred_binary, average=None),\n",
    "        'f1': f1_score(y_true, y_pred_binary, average=None),\n",
    "        'auc': [roc_auc_score(y_true[:, i], y_pred_proba[:, i]) for i in range(n_classes)]\n",
    "    }\n",
    "    metrics['class_metrics'] = class_metrics\n",
    "    \n",
    "    print(\"\\n=== VulnAnalyzer Test Results (Optimal Thresholds) ===\")\n",
    "    print(f\"Test Samples: {len(X_test)}\")\n",
    "    print(f\"Average Precision: {metrics['avg_precision']:.4f}\")\n",
    "    print(f\"Average Recall: {metrics['avg_recall']:.4f}\")\n",
    "    print(f\"Average F1-Score: {metrics['avg_f1']:.4f}\")\n",
    "    print(f\"Average AUC: {metrics['avg_auc']:.4f}\")\n",
    "    \n",
    "    if detailed_report:\n",
    "        print(\"\\nPer-Class Metrics with Optimal Thresholds:\")\n",
    "        for i in range(n_classes):\n",
    "            print(f\"\\nVulnerability {i} (Threshold: {optimal_thresholds[i]:.3f}):\")\n",
    "            print(f\"Precision: {class_metrics['precision'][i]:.4f}\")\n",
    "            print(f\"Recall: {class_metrics['recall'][i]:.4f}\")\n",
    "            print(f\"F1-Score: {class_metrics['f1'][i]:.4f}\")\n",
    "            print(f\"AUC: {class_metrics['auc'][i]:.4f}\")\n",
    "    \n",
    "    return metrics, y_pred_proba, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c627c3-8e44-4dd4-b047-f43341839365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2130, Train F1: 0.2477, Threshold: 0.700\n",
      "Epoch 10, Loss: 0.1333, Train F1: 0.4322, Threshold: 0.700\n",
      "Epoch 20, Loss: 0.0962, Train F1: 0.5590, Threshold: 0.700\n",
      "Epoch 30, Loss: 0.0743, Train F1: 0.6513, Threshold: 0.700\n",
      "Epoch 40, Loss: 0.0608, Train F1: 0.7113, Threshold: 0.700\n",
      "Epoch 50, Loss: 0.0522, Train F1: 0.7565, Threshold: 0.700\n",
      "Epoch 60, Loss: 0.0466, Train F1: 0.7776, Threshold: 0.700\n",
      "Epoch 70, Loss: 0.0414, Train F1: 0.8059, Threshold: 0.700\n",
      "Epoch 80, Loss: 0.0383, Train F1: 0.8175, Threshold: 0.700\n",
      "Epoch 90, Loss: 0.0348, Train F1: 0.8361, Threshold: 0.700\n",
      "Epoch 100, Loss: 0.0333, Train F1: 0.8425, Threshold: 0.700\n",
      "Epoch 110, Loss: 0.0311, Train F1: 0.8540, Threshold: 0.700\n",
      "Epoch 120, Loss: 0.0296, Train F1: 0.8639, Threshold: 0.700\n",
      "Epoch 130, Loss: 0.0285, Train F1: 0.8690, Threshold: 0.700\n",
      "Epoch 140, Loss: 0.0280, Train F1: 0.8704, Threshold: 0.700\n",
      "Epoch 150, Loss: 0.0269, Train F1: 0.8756, Threshold: 0.700\n",
      "Epoch 160, Loss: 0.0266, Train F1: 0.8733, Threshold: 0.700\n",
      "Epoch 170, Loss: 0.0260, Train F1: 0.8751, Threshold: 0.700\n",
      "Epoch 180, Loss: 0.0256, Train F1: 0.8762, Threshold: 0.700\n",
      "Epoch 190, Loss: 0.0261, Train F1: 0.8751, Threshold: 0.700\n",
      "Early stopping at epoch 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m_/9b11rh5j3kjfpgjn4y9twzv40000gn/T/ipykernel_88424/16465563.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VulnAnalyzer Test Results (Optimal Thresholds) ===\n",
      "Test Samples: 1074\n",
      "Average Precision: 0.8071\n",
      "Average Recall: 0.7729\n",
      "Average F1-Score: 0.7896\n",
      "Average AUC: 0.9263\n",
      "\n",
      "Per-Class Metrics with Optimal Thresholds:\n",
      "\n",
      "Vulnerability 0 (Threshold: 0.363):\n",
      "Precision: 0.8333\n",
      "Recall: 0.9059\n",
      "F1-Score: 0.8681\n",
      "AUC: 0.9202\n",
      "\n",
      "Vulnerability 1 (Threshold: 0.627):\n",
      "Precision: 0.9592\n",
      "Recall: 0.6528\n",
      "F1-Score: 0.7769\n",
      "AUC: 0.9147\n",
      "\n",
      "Vulnerability 2 (Threshold: 0.851):\n",
      "Precision: 0.5217\n",
      "Recall: 0.6667\n",
      "F1-Score: 0.5854\n",
      "AUC: 0.9590\n",
      "\n",
      "Vulnerability 3 (Threshold: 0.792):\n",
      "Precision: 0.6471\n",
      "Recall: 0.6111\n",
      "F1-Score: 0.6286\n",
      "AUC: 0.9557\n",
      "\n",
      "Vulnerability 4 (Threshold: 0.660):\n",
      "Precision: 0.8966\n",
      "Recall: 0.4407\n",
      "F1-Score: 0.5909\n",
      "AUC: 0.8710\n",
      "\n",
      "Vulnerability 5 (Threshold: 0.657):\n",
      "Precision: 0.5312\n",
      "Recall: 0.4595\n",
      "F1-Score: 0.4928\n",
      "AUC: 0.8771\n",
      "\n",
      "Vulnerability 6 (Threshold: 0.540):\n",
      "Precision: 0.8689\n",
      "Recall: 0.7833\n",
      "F1-Score: 0.8238\n",
      "AUC: 0.9301\n",
      "\n",
      "Vulnerability 7 (Threshold: 0.591):\n",
      "Precision: 0.5660\n",
      "Recall: 0.5769\n",
      "F1-Score: 0.5714\n",
      "AUC: 0.9318\n",
      "\n",
      "Vulnerability 8 (Threshold: 0.538):\n",
      "Precision: 0.8681\n",
      "Recall: 0.7783\n",
      "F1-Score: 0.8208\n",
      "AUC: 0.9241\n",
      "\n",
      "Sample Probabilities (first 5): [[0.2172857  0.2511046  0.17601858 0.22959015 0.22682369 0.6925069\n",
      "  0.38925922 0.26814964 0.38523892]\n",
      " [0.19193916 0.23151605 0.16942258 0.22042702 0.849122   0.35374895\n",
      "  0.21575409 0.29764798 0.19654275]\n",
      " [0.16376656 0.2366532  0.12543856 0.1850127  0.23490629 0.24098462\n",
      "  0.41332054 0.4802216  0.41199642]\n",
      " [0.7522641  0.2515681  0.08232015 0.15689369 0.59218544 0.34490234\n",
      "  0.22536965 0.16829999 0.24004413]\n",
      " [0.67010653 0.46348518 0.10385582 0.16769661 0.13039558 0.39067638\n",
      "  0.21375364 0.42192322 0.24480319]]\n",
      "Attention Weights Shape: None (CNN)\n"
     ]
    }
   ],
   "source": [
    "# Final training and testing\n",
    "validator_feedback = {0: 0.5, 2: 0.3}\n",
    "final_model = train_vuln_analyzer(\n",
    "    X_train, train_prob, y_train,\n",
    "    threshold=0.7, epochs=200, lr=0.0001, dropout_rate=0.2, weight_decay=0.001,\n",
    "    validator_feedback=validator_feedback\n",
    ")\n",
    "metrics, probabilities, attn_weights = test_vuln_analyzer(final_model, X_test, test_prob, y_test)\n",
    "\n",
    "print(\"\\nSample Probabilities (first 5):\", probabilities[:5])\n",
    "print(\"Attention Weights Shape:\", attn_weights.shape if attn_weights is not None else \"None (CNN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0745a073-cc56-4590-adfd-36a5d5881b43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(final_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../codebert/vuln_analyzer_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(final_model, '../codebert/vuln_analyzer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462cf973-38d4-44d2-8a2d-cc4597feae88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
