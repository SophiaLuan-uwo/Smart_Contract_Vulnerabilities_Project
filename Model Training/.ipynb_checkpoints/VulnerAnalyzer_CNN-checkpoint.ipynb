{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c4e0e-69b2-4351-8e25-0293bd6ef7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, confusion_matrix, precision_score, f1_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# Check for MPS availability (Apple Silicon GPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(\"../Codebert/train_data.csv\")\n",
    "df_test = pd.read_csv(\"../Codebert/test_data.csv\")\n",
    "\n",
    "X_train = torch.load(\"../CodeT5/X_train.pt\", weights_only=False)\n",
    "X_test = torch.load(\"../CodeT5/X_test.pt\", weights_only=False)\n",
    "\n",
    "# Process vulnerability_list (9 dimensions)\n",
    "def process_vulnerability_list(vuln_list_series, num_classes=9):\n",
    "    vuln_lists = vuln_list_series.apply(ast.literal_eval)\n",
    "    y_binary = np.array([np.array(vuln) for vuln in vuln_lists], dtype=np.float32)\n",
    "    if y_binary.shape[1] != num_classes:\n",
    "        raise ValueError(f\"Expected {num_classes} dimensions, got {y_binary.shape[1]}\")\n",
    "    return torch.tensor(y_binary, dtype=torch.float32)\n",
    "\n",
    "y_train = process_vulnerability_list(df_train['vulnerability_list'], num_classes=9)\n",
    "y_test = process_vulnerability_list(df_test['vulnerability_list'], num_classes=9)\n",
    "\n",
    "torch.save(y_train, \"../Codebert/y_train.pt\")\n",
    "torch.save(y_test, \"../Codebert/y_test.pt\")\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ee5e0-56f1-4a50-84d0-a3b3eec8663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VulnScreener and get probabilities\n",
    "class VulnScreener(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VulnScreener, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # Input layer to Hidden Layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),  # Hidden Layer 1 to Hidden Layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)    # Hidden Layer 2 to Output Layer\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)  # Forward pass through the network\n",
    "        \n",
    "screener = torch.load('../Codebert/vuln_screener_model.pth', weights_only=False)\n",
    "screener.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_prob = screener(X_train)\n",
    "    test_prob = screener(X_test)\n",
    "\n",
    "# Enhanced oversampling with noise injection\n",
    "def oversample_rare_classes(X, p_s, y, class_indices=[2, 3, 7], multiplier=5, noise_level=0.01):\n",
    "    mask = torch.any(y[:, class_indices] == 1, dim=1)\n",
    "    X_rare = X[mask]\n",
    "    p_s_rare = p_s[mask]\n",
    "    y_rare = y[mask]\n",
    "    for _ in range(multiplier):\n",
    "        noise = torch.randn_like(X_rare) * noise_level\n",
    "        X = torch.cat([X, X_rare + noise], dim=0)\n",
    "        p_s = torch.cat([p_s, p_s_rare], dim=0)\n",
    "        y = torch.cat([y, y_rare], dim=0)\n",
    "    return X, p_s, y\n",
    "\n",
    "X_train, train_prob, y_train = oversample_rare_classes(X_train, train_prob, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5795912-9a3c-474d-95e0-685c032dee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Focal Loss\n",
    "class WeightedFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2):\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha if alpha is not None else torch.ones(9).to(device)  # Default equal weights\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCELoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "# Enhanced VulnAnalyzer with corrected residual connection\n",
    "class VulnAnalyzer(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(VulnAnalyzer, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)  # 769 -> 384\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)  # 384 -> 192\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)  # 192 -> 96\n",
    "        )\n",
    "        # Adjust residual path to match output size of conv3 (256 channels, 96 length)\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv1d(1, 256, kernel_size=1),\n",
    "            nn.AvgPool1d(kernel_size=8, stride=8)  # Downsample 769 to ~96\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256 * 96, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 9),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, p_s):\n",
    "        if p_s.dim() == 1:\n",
    "            p_s = p_s.unsqueeze(1)\n",
    "        x = torch.cat((x, p_s), dim=1).unsqueeze(1)  # [batch_size, 1, 769]\n",
    "        residual = self.residual(x)  # [batch_size, 256, 96]\n",
    "        x = self.conv1(x)  # [batch_size, 64, 384]\n",
    "        x = self.conv2(x)  # [batch_size, 128, 192]\n",
    "        x = self.conv3(x)  # [batch_size, 256, 96]\n",
    "        # Ensure residual matches xâ€™s size\n",
    "        if residual.size(2) != x.size(2):\n",
    "            residual = nn.functional.interpolate(residual, size=x.size(2), mode='nearest')\n",
    "        x = x + residual  # Residual connection\n",
    "        x = x.view(x.size(0), -1)  # [batch_size, 256 * 96]\n",
    "        x = self.fc_layers(x)\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06c61d-b7b6-4d43-bee2-d3c9674e3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with early stopping\n",
    "def train_vuln_analyzer(X_train, p_s_train, y_train, threshold=0.7, epochs=200, lr=0.0001, \n",
    "                       dropout_rate=0.2, weight_decay=0.001, validator_feedback=None, patience=20):\n",
    "    model = VulnAnalyzer(dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    # Compute class weights based on inverse frequency\n",
    "    class_counts = y_train.sum(dim=0)\n",
    "    alpha = 1.0 / (class_counts + 1e-6)\n",
    "    alpha = alpha / alpha.sum() * 9  # Normalize to sum to 9\n",
    "    criterion = WeightedFocalLoss(alpha=alpha.to(device), gamma=2)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    X_train, p_s_train, y_train = X_train.to(device), p_s_train.to(device), y_train.to(device)\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        mask = p_s_train > threshold\n",
    "        X_train_filtered = X_train[mask.squeeze()]\n",
    "        p_s_train_filtered = p_s_train[mask]\n",
    "        y_train_filtered = y_train[mask.squeeze()]\n",
    "        \n",
    "        if len(X_train_filtered) == 0:\n",
    "            threshold = max(0.1, threshold - 0.1)\n",
    "            mask = p_s_train > threshold\n",
    "            X_train_filtered = X_train[mask.squeeze()]\n",
    "            p_s_train_filtered = p_s_train[mask]\n",
    "            y_train_filtered = y_train[mask.squeeze()]\n",
    "        \n",
    "        p_a, _ = model(X_train_filtered, p_s_train_filtered)\n",
    "        loss = criterion(p_a, y_train_filtered)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred_binary = (p_a > 0.5).float()\n",
    "            train_f1 = f1_score(y_train_filtered.cpu(), y_pred_binary.cpu(), average='micro')\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if validator_feedback is not None and epoch % 10 == 0 and epoch > 0:\n",
    "            with torch.no_grad():\n",
    "                for vuln_idx, correction_factor in validator_feedback.items():\n",
    "                    if vuln_idx < 9 and correction_factor > 0:\n",
    "                        model.fc_layers[-2].weight[vuln_idx] *= (1 + correction_factor * 0.1)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Train F1: {train_f1:.4f}, Threshold: {threshold:.3f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if train_f1 > best_f1:\n",
    "            best_f1 = train_f1\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384556e7-c653-4ae4-981b-5a592bc1687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced testing function\n",
    "def test_vuln_analyzer(model, X_test, p_s_test, y_test, detailed_report=True):\n",
    "    model.eval()\n",
    "    X_test, p_s_test, y_test = X_test.to(device), p_s_test.to(device), y_test.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        p_a, attn_weights = model(X_test, p_s_test)\n",
    "        y_pred_proba = p_a.cpu().numpy()\n",
    "        y_true = y_test.cpu().numpy()\n",
    "    \n",
    "    metrics = {}\n",
    "    n_classes = y_true.shape[1]\n",
    "    optimal_thresholds = []\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true[:, i], y_pred_proba[:, i])\n",
    "        f1_scores = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "        optimal_idx = np.argmax(f1_scores)\n",
    "        optimal_thresholds.append(thresholds[optimal_idx])\n",
    "    \n",
    "    y_pred_binary = np.zeros_like(y_pred_proba)\n",
    "    for i in range(n_classes):\n",
    "        y_pred_binary[:, i] = (y_pred_proba[:, i] >= optimal_thresholds[i]).astype(int)\n",
    "    \n",
    "    metrics['avg_precision'] = precision_score(y_true, y_pred_binary, average='micro')\n",
    "    metrics['avg_recall'] = recall_score(y_true, y_pred_binary, average='micro')\n",
    "    metrics['avg_f1'] = f1_score(y_true, y_pred_binary, average='micro')\n",
    "    metrics['avg_auc'] = roc_auc_score(y_true, y_pred_proba, average='micro')\n",
    "    metrics['optimal_thresholds'] = optimal_thresholds\n",
    "    \n",
    "    class_metrics = {\n",
    "        'precision': precision_score(y_true, y_pred_binary, average=None, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred_binary, average=None),\n",
    "        'f1': f1_score(y_true, y_pred_binary, average=None),\n",
    "        'auc': [roc_auc_score(y_true[:, i], y_pred_proba[:, i]) for i in range(n_classes)]\n",
    "    }\n",
    "    metrics['class_metrics'] = class_metrics\n",
    "    \n",
    "    print(\"\\n=== VulnAnalyzer Test Results (Optimal Thresholds) ===\")\n",
    "    print(f\"Test Samples: {len(X_test)}\")\n",
    "    print(f\"Average Precision: {metrics['avg_precision']:.4f}\")\n",
    "    print(f\"Average Recall: {metrics['avg_recall']:.4f}\")\n",
    "    print(f\"Average F1-Score: {metrics['avg_f1']:.4f}\")\n",
    "    print(f\"Average AUC: {metrics['avg_auc']:.4f}\")\n",
    "    \n",
    "    if detailed_report:\n",
    "        print(\"\\nPer-Class Metrics with Optimal Thresholds:\")\n",
    "        for i in range(n_classes):\n",
    "            print(f\"\\nVulnerability {i} (Threshold: {optimal_thresholds[i]:.3f}):\")\n",
    "            print(f\"Precision: {class_metrics['precision'][i]:.4f}\")\n",
    "            print(f\"Recall: {class_metrics['recall'][i]:.4f}\")\n",
    "            print(f\"F1-Score: {class_metrics['f1'][i]:.4f}\")\n",
    "            print(f\"AUC: {class_metrics['auc'][i]:.4f}\")\n",
    "    \n",
    "    return metrics, y_pred_proba, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c627c3-8e44-4dd4-b047-f43341839365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training and testing\n",
    "validator_feedback = {0: 0.5, 2: 0.3}\n",
    "final_model = train_vuln_analyzer(\n",
    "    X_train, train_prob, y_train,\n",
    "    threshold=0.7, epochs=200, lr=0.0001, dropout_rate=0.2, weight_decay=0.001,\n",
    "    validator_feedback=validator_feedback\n",
    ")\n",
    "metrics, probabilities, attn_weights = test_vuln_analyzer(final_model, X_test, test_prob, y_test)\n",
    "\n",
    "print(\"\\nSample Probabilities (first 5):\", probabilities[:5])\n",
    "print(\"Attention Weights Shape:\", attn_weights.shape if attn_weights is not None else \"None (CNN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745a073-cc56-4590-adfd-36a5d5881b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model, '../Codebert/vuln_analyzer_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
